{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習特論 第6回課題 決定木、ランダムフォレスト、アダブーストで アヤメのデータ、MNIST, Fashion-MNIST の分類を比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google colab で実行する場合は、次の行の先頭の # を削除してこのブロックを実行する\n",
    "#!pip install japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearnデータセットに収録されたiris(アヤメ)のデータセットをロードしてデータフレームを作成\n",
    "def load_iris_data():\n",
    "    data = load_iris()\n",
    "    x = pd.DataFrame(data[\"data\"],columns=data[\"feature_names\"])\n",
    "    y = pd.DataFrame(data[\"target\"],columns=[\"target\"])\n",
    "    return x, y\n",
    "\n",
    "# 手書き文字のデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_mnist_data():\n",
    "    data = fetch_openml('mnist_784', parser='auto', version=1)\n",
    "    _x = np.array(data['data'].astype(np.float32))\n",
    "    _y = np.array(data['target'].astype(np.int32))\n",
    "    _, x, _, y = train_test_split(_x, _y, test_size=0.1, random_state=1, stratify=_y)\n",
    "    return x, y\n",
    "\n",
    "# Fashion-MNISTデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_fashion_mnist_data():\n",
    "    data = fetch_openml('Fashion-MNIST', parser='auto')\n",
    "    _x = np.array(data['data'].astype(np.float32))\n",
    "    _y = np.array(data['target'].astype(np.int32))\n",
    "    _, x, _, y = train_test_split(_x, _y, test_size=0.1, random_state=1, stratify=_y) \n",
    "    return x, y\n",
    "\n",
    "# 一括処理のためにデータセットの辞書を作成\n",
    "dataset = {'iris': load_iris_data(), 'mnist': load_mnist_data(), 'fashon-mnist': load_fashion_mnist_data()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一括処理のためにモデルの辞書を作成\n",
    "model = {\n",
    "    # k近傍法のモデル\n",
    "    'kNN(k=3)':\n",
    "    KNeighborsClassifier(n_neighbors=3, # k を指定 (デフォルトは 5)\n",
    "                         weights='uniform',  # 距離を考慮しない(uniform:デフォルト)、する(distance)\n",
    "                         algorithm='auto', # 近傍点計算アルゴリズム (auto:デフォルト,ball_tree,kd_tree,brute)\n",
    "                         leaf_size=30,  # ball_tree,kd_tree指定時のリーフサイズの設定 (デフォルトは 30)\n",
    "                         p=2),  # 距離計算の次元 (2:デフォルト、1)\n",
    "    # svm (kernel=\"linear\", C=1.0) のモデル\n",
    "    'SVC(kernel=\"linear\", C=1)':\n",
    "    svm.SVC(kernel=\"linear\", C=1, max_iter=100000, verbose=True, random_state=1),\n",
    "    # svm (kernel=\"rbf\", C=1) のモデル\n",
    "    'SVC(kernel=\"rbf\", C=1)':\n",
    "    svm.SVC(kernel=\"rbf\", C=1, max_iter=100000, verbose=True, random_state=1),\n",
    "    # 決定木\n",
    "    'DecisionTree(max_depth=10)':\n",
    "    DecisionTreeClassifier(max_depth=10, # 木の深さの最大\n",
    "                                random_state=2), # 乱数シード\n",
    "    # ランダムフォレストのモデル\n",
    "    'randomforest(max_depth=10, n_estimators=10)':\n",
    "    RandomForestClassifier(max_depth=10, # 木の深さの最大\n",
    "                             n_estimators=10, # 木の数\n",
    "                             random_state=2), # 乱数シード\n",
    "    # # アダブーストのモデル (実行時間がかかる場合は n_estimators を減らす)\n",
    "    'Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50)':\n",
    "    AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10, random_state=1), # ベースモデルを指定 (scikit learn のバージョンにより書式が異なります)\n",
    "                            n_estimators=50, # 木の数\n",
    "                            random_state=1), # 乱数シード\n",
    "    'Adaboost(SVC, n_estimators=10)':\n",
    "    AdaBoostClassifier(estimator=svm.SVC(kernel=\"rbf\", C=1, max_iter=100000, verbose=True, random_state=1), # ベースモデルを指定 (scikit learn のバージョンにより書式が異なります)\n",
    "                            n_estimators=10, # 木の数\n",
    "                            random_state=1, # 乱数シード\n",
    "                            algorithm='SAMME'), # SVCを使うために AdaBoost のアルゴリズムを指定\n",
    "    # 'Adaboost(dct(max_depth=10), n_estimators=170)':\n",
    "    # AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=10, random_state=1), # ベースモデルを指定 (scikit learn のバージョンにより書式が異なります)\n",
    "    #                         n_estimators=170, # 木の数\n",
    "    #                         random_state=1), # 乱数シード\n",
    "    # 勾配ブースティングのモデル (実行時間がかかる場合は n_estimators を減らすか HistGradientBoostingClassifier に変更)\n",
    "    # 'GradientBoostingClassifier(max_depth=5, n_estimators=50)':\n",
    "    # GradientBoostingClassifier(max_depth=5, # 木の深さの最大\n",
    "    #                            n_estimators=50, # 木の数\n",
    "    #                            random_state=1), # 乱数シード\n",
    "    # 'GradientBoostingClassifier(max_depth=5, n_estimators=170)':\n",
    "    # GradientBoostingClassifier(max_depth=5, # 木の深さの最大\n",
    "    #                            n_estimators=170, # 木の数\n",
    "    #                            random_state=1), # 乱数シード\n",
    "    'HistGradientBoostingClassifier(max_depth=5)':\n",
    "    HistGradientBoostingClassifier(max_depth=5, # 木の深さの最大\n",
    "                               random_state=1), # 乱数シード\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:iris  x_train:112 x_test:38 y_train:112 y_test:38\n",
      "dataset:iris model:kNN(k=3) accuracy_score: train_data: 0.96429 test_data: 0.97368\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.97321 test_data: 1.0\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.96429 test_data: 0.97368\n",
      "dataset:iris model:DecisionTree(max_depth=10) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:iris model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.99107 test_data: 0.97368\n",
      "dataset:iris model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:iris model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.33929 test_data: 0.31579\n",
      "dataset:iris model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "## dataset:mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "dataset:mnist model:kNN(k=3) accuracy_score: train_data: 0.96686 test_data: 0.93371\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.90971\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.9861 test_data: 0.94971\n",
      "dataset:mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.95619 test_data: 0.77543\n",
      "dataset:mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.97962 test_data: 0.87886\n",
      "dataset:mnist model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.92686\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:mnist model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.11257 test_data: 0.11257\n",
      "dataset:mnist model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.93943\n",
      "## dataset:fashon-mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "dataset:fashon-mnist model:kNN(k=3) accuracy_score: train_data: 0.89295 test_data: 0.80286\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.79429\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.89638 test_data: 0.83429\n",
      "dataset:fashon-mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.90895 test_data: 0.73029\n",
      "dataset:fashon-mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.94 test_data: 0.80343\n",
      "dataset:fashon-mnist model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.81543\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:fashon-mnist model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.69181 test_data: 0.68343\n",
      "dataset:fashon-mnist model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.83943\n"
     ]
    }
   ],
   "source": [
    "# 辞書に格納したデータセットそれぞれについて性能を確認\n",
    "for dataset_key in dataset.keys():\n",
    "    # データを学習用と検証用に分割\n",
    "    x, y = dataset[dataset_key]\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "    print(f'## dataset:{dataset_key} ',\n",
    "          f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデルを学習\n",
    "        clf = model[model_key]\n",
    "        clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "        # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'dataset:{dataset_key} model:{model_key}', \n",
    "            f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:iris  x_train:112 x_test:38 y_train:112 y_test:38\n",
      "# no scaling\n",
      "dataset:iris model:kNN(k=3) accuracy_score: train_data: 0.96429 test_data: 0.97368\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.97321 test_data: 1.0\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.96429 test_data: 0.97368\n",
      "dataset:iris model:DecisionTree(max_depth=10) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:iris model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.99107 test_data: 0.97368\n",
      "dataset:iris model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:iris model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.33929 test_data: 0.31579\n",
      "dataset:iris model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "# with scaling\n",
      "dataset:iris model:kNN(k=3) accuracy_score: train_data: 0.95536 test_data: 0.94737\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.97321 test_data: 0.97368\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.97321 test_data: 0.97368\n",
      "dataset:iris model:DecisionTree(max_depth=10) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:iris model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.99107 test_data: 0.97368\n",
      "dataset:iris model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:iris model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.33929 test_data: 0.31579\n",
      "dataset:iris model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "## dataset:mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "# no scaling\n",
      "dataset:mnist model:kNN(k=3) accuracy_score: train_data: 0.96686 test_data: 0.93371\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.90971\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.9861 test_data: 0.94971\n",
      "dataset:mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.95619 test_data: 0.77543\n",
      "dataset:mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.97962 test_data: 0.87886\n",
      "dataset:mnist model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.92686\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:mnist model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.11257 test_data: 0.11257\n",
      "dataset:mnist model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.93943\n",
      "# with scaling\n",
      "dataset:mnist model:kNN(k=3) accuracy_score: train_data: 0.9459 test_data: 0.89771\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.91371\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.98552 test_data: 0.92743\n",
      "dataset:mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.95619 test_data: 0.77543\n",
      "dataset:mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.97962 test_data: 0.87886\n",
      "dataset:mnist model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.92686\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:mnist model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.11257 test_data: 0.11257\n",
      "dataset:mnist model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.93943\n",
      "## dataset:fashon-mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "# no scaling\n",
      "dataset:fashon-mnist model:kNN(k=3) accuracy_score: train_data: 0.89295 test_data: 0.80286\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.79429\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.89638 test_data: 0.83429\n",
      "dataset:fashon-mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.90895 test_data: 0.73029\n",
      "dataset:fashon-mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.94 test_data: 0.80343\n",
      "dataset:fashon-mnist model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.81543\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:fashon-mnist model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.69181 test_data: 0.68343\n",
      "dataset:fashon-mnist model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.83943\n",
      "# with scaling\n",
      "dataset:fashon-mnist model:kNN(k=3) accuracy_score: train_data: 0.89352 test_data: 0.8\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.80057\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.91429 test_data: 0.82571\n",
      "dataset:fashon-mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.90895 test_data: 0.73086\n",
      "dataset:fashon-mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.93981 test_data: 0.804\n",
      "dataset:fashon-mnist model:Adaboost(DecisionTreeClassifier, dct(max_depth=10), n_estimators=50) accuracy_score: train_data: 1.0 test_data: 0.816\n",
      "[LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM][LibSVM]dataset:fashon-mnist model:Adaboost(SVC, n_estimators=10) accuracy_score: train_data: 0.68324 test_data: 0.67029\n",
      "dataset:fashon-mnist model:HistGradientBoostingClassifier(max_depth=5) accuracy_score: train_data: 1.0 test_data: 0.83943\n"
     ]
    }
   ],
   "source": [
    "# 辞書に格納したデータセットそれぞれについて性能を確認\n",
    "for dataset_key in dataset.keys():\n",
    "    # データを学習用と検証用に分割\n",
    "    x, y = dataset[dataset_key]\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.25, random_state=1, stratify=y) # 検証用データに25%を割当て\n",
    "    print(f'## dataset:{dataset_key} ',\n",
    "          f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "    # データ標準化なしで性能を測定\n",
    "    print('# no scaling')\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデルを学習\n",
    "        clf = model[model_key]\n",
    "        clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "        # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'dataset:{dataset_key} model:{model_key}', \n",
    "            f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')\n",
    "\n",
    "    # データを標準化\n",
    "    print('# with scaling')\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデルを学習\n",
    "        clf = model[model_key]\n",
    "        clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "        # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'dataset:{dataset_key} model:{model_key}', \n",
    "            f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
