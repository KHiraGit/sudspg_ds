{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習特論 第6回課題 モデル・パラメータの探索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google colab で実行する場合は、次の行の先頭の # を削除してこのブロックを実行する\n",
    "#!pip install japanize-matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fashion-MNISTデータセットをダウンロードして、実験用データを準備 (70000枚のうち7000枚を利用)\n",
    "def load_fashion_mnist_data():\n",
    "    data = fetch_openml('Fashion-MNIST')\n",
    "    _x = np.array(data['data'].astype(np.float32))\n",
    "    _y = np.array(data['target'].astype(np.int32))\n",
    "    _, x, _, y = train_test_split(_x, _y, test_size=0.1, random_state=1, stratify=_y) \n",
    "    return x, y\n",
    "\n",
    "# 一括処理のためにデータセットの辞書を作成\n",
    "dataset = {'fashon-mnist': load_fashion_mnist_data()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル・パラメータ探索のためにモデル・パラメータ探索範囲の辞書を作成\n",
    "model = {\n",
    "    # k近傍法のモデル\n",
    "    'kNN': {'model': KNeighborsClassifier(),\n",
    "            'parameters': {'n_neighbors': [3,5,7,9], # k の範囲\n",
    "                           'p': [1,2]},  # 距離計算の次元 (2:デフォルト、1)\n",
    "           },\n",
    "    # svm (kernel=\"linear\", C=1.0) のモデル\n",
    "    'SVC': {'model': svm.SVC(),\n",
    "            'parameters': {'kernel': [\"linear\", \"rbf\"],\n",
    "                           'C': [0.1,1,10],}, \n",
    "           },\n",
    "    # 決定木\n",
    "    'DecisionTree:':{'model': DecisionTreeClassifier(random_state=1),\n",
    "                     'parameters': {'max_depth': [5,10,15]}, # 木の深さの最大\n",
    "                    },\n",
    "    # ランダムフォレスト\n",
    "    'randomforest':{'model': RandomForestClassifier(random_state=1),\n",
    "                    'parameters': {'max_depth': [3,5,10], # 木の深さの最大\n",
    "                                   'n_estimators': [10,20,30]}, # 木の数\n",
    "                   },\n",
    "    # アダブーストのモデル\n",
    "    'Adaboost':{'model': AdaBoostClassifier(random_state=1),\n",
    "                'parameters': {'base_estimator': [DecisionTreeClassifier(max_depth=10, random_state=1)], # ベースモデル\n",
    "                               'n_estimators': [50,100,150,200]}, # 木の数\n",
    "               },\n",
    "    # 勾配ブースティングのモデル\n",
    "    'GradientBoostingClassifier':{'model': GradientBoostingClassifier(random_state=1),\n",
    "                                  'parameters': {'max_depth': [3,5,10], # 木の深さの最大\n",
    "                                  'n_estimators': [100,200,300]}, # 木の数\n",
    "                                  },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:fashon-mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n"
     ]
    }
   ],
   "source": [
    "# 辞書に格納したデータセットそれぞれについて性能を確認\n",
    "for dataset_key in dataset.keys():\n",
    "    # データを学習用と検証用に分割\n",
    "    x, y = dataset[dataset_key]\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.25, random_state=1) # 検証用データに25%を割当て\n",
    "    print(f'## dataset:{dataset_key} ',\n",
    "          f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデル・パラメータを探索\n",
    "        clf = model[model_key]['model']\n",
    "        parameters = model[model_key]['parameters']\n",
    "        clf_search = GridSearchCV(clf, parameters)\n",
    "        clf_search.fit(x_train, np.array(y_train).ravel())\n",
    "\n",
    "        # 探索したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf_search.best_estimator_.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf_search.best_estimator_.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'model:{model_key} best parameters:', clf_search.best_params_,\n",
    "              f'model:{model_key} accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## dataset:iris  x_train:112 x_test:38 y_train:112 y_test:38\n",
      "# no scaling\n",
      "dataset:iris model:kNN(k=3) accuracy_score: train_data: 0.94643 test_data: 1.0\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.98214 test_data: 1.0\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.97321 test_data: 0.97368\n",
      "dataset:iris model:DecisionTree(max_depth=10) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:iris model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.99107 test_data: 0.97368\n",
      "dataset:iris model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:iris model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "# with scaling\n",
      "dataset:iris model:kNN(k=3) accuracy_score: train_data: 0.94643 test_data: 1.0\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.96429 test_data: 0.97368\n",
      "[LibSVM]dataset:iris model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.97321 test_data: 0.97368\n",
      "dataset:iris model:DecisionTree(max_depth=10) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:iris model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.99107 test_data: 0.97368\n",
      "dataset:iris model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "dataset:iris model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.97368\n",
      "## dataset:mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "# no scaling\n",
      "dataset:mnist model:kNN(k=3) accuracy_score: train_data: 0.96686 test_data: 0.91829\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.89371\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.98381 test_data: 0.944\n",
      "dataset:mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.95543 test_data: 0.75829\n",
      "dataset:mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.98286 test_data: 0.88171\n",
      "dataset:mnist model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.92286\n",
      "dataset:mnist model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.924\n",
      "# with scaling\n",
      "dataset:mnist model:kNN(k=3) accuracy_score: train_data: 0.94476 test_data: 0.88857\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.88914\n",
      "[LibSVM]dataset:mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.98095 test_data: 0.91429\n",
      "dataset:mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.95543 test_data: 0.75829\n",
      "dataset:mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.98286 test_data: 0.88171\n",
      "dataset:mnist model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.92286\n",
      "dataset:mnist model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.924\n",
      "## dataset:fashon-mnist  x_train:5250 x_test:1750 y_train:5250 y_test:1750\n",
      "# no scaling\n",
      "dataset:fashon-mnist model:kNN(k=3) accuracy_score: train_data: 0.89467 test_data: 0.80857\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 1.0 test_data: 0.81086\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.89371 test_data: 0.84571\n",
      "dataset:fashon-mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.89124 test_data: 0.76686\n",
      "dataset:fashon-mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.93067 test_data: 0.82629\n",
      "dataset:fashon-mnist model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.84057\n",
      "dataset:fashon-mnist model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.85886\n",
      "# with scaling\n",
      "dataset:fashon-mnist model:kNN(k=3) accuracy_score: train_data: 0.89619 test_data: 0.81371\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"linear\", C=1) accuracy_score: train_data: 0.99981 test_data: 0.80514\n",
      "[LibSVM]dataset:fashon-mnist model:SVC(kernel=\"rbf\", C=1) accuracy_score: train_data: 0.91562 test_data: 0.84629\n",
      "dataset:fashon-mnist model:DecisionTree(max_depth=10) accuracy_score: train_data: 0.89124 test_data: 0.76686\n",
      "dataset:fashon-mnist model:randomforest(max_depth=10, n_estimators=10) accuracy_score: train_data: 0.93086 test_data: 0.82629\n",
      "dataset:fashon-mnist model:Adaboost(dct(max_depth=10), n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.84114\n",
      "dataset:fashon-mnist model:GradientBoostingClassifier(max_depth=5, n_estimators=170) accuracy_score: train_data: 1.0 test_data: 0.85886\n"
     ]
    }
   ],
   "source": [
    "# 辞書に格納したデータセットそれぞれについて性能を確認\n",
    "for dataset_key in dataset.keys():\n",
    "    # データを学習用と検証用に分割\n",
    "    x, y = dataset[dataset_key]\n",
    "    x_train, x_test, y_train, y_test = \\\n",
    "        train_test_split(x, y, test_size=0.25, random_state=1) # 検証用データに25%を割当て\n",
    "    print(f'## dataset:{dataset_key} ',\n",
    "          f'x_train:{len(x_train)} x_test:{len(x_test)} y_train:{len(y_train)} y_test:{len(y_test)}')\n",
    "\n",
    "    # データ標準化なしで性能を測定\n",
    "    print('# no scaling')\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデルを学習\n",
    "        clf = model[model_key]\n",
    "        clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "        # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'dataset:{dataset_key} model:{model_key}', \n",
    "            f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')\n",
    "\n",
    "    # データを標準化\n",
    "    print('# with scaling')\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)\n",
    "\n",
    "    # 辞書に格納したモデルそれぞれについて性能を測定\n",
    "    for model_key in model.keys():\n",
    "        # 学習用データを利用してモデルを学習\n",
    "        clf = model[model_key]\n",
    "        clf = clf.fit(x_train, np.array(y_train).ravel()) \n",
    "\n",
    "        # 学習したモデルの性能(正答率)を学習用データと検証用データで評価\n",
    "        predict_train = clf.predict(x_train)\n",
    "        train_score = metrics.accuracy_score(y_train, predict_train)\n",
    "        predict_test = clf.predict(x_test)\n",
    "        test_score = metrics.accuracy_score(y_test, predict_test)\n",
    "        print(f'dataset:{dataset_key} model:{model_key}', \n",
    "            f'accuracy_score: train_data:{train_score: 0.5} test_data:{test_score: 0.5}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "18ef0afcf35f1452430268c7ef685ac367525865953635d3b087fb3264879c09"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
